{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from speechpy.feature import mfcc\n",
    "\n",
    "mean_signal_length = 32000  # Empirically calculated for the given data set\n",
    "import sys\n",
    "from typing import Tuple\n",
    "import glob\n",
    "import numpy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "def classLabels(s):\n",
    "    if(s=='neu'):\n",
    "        return 0\n",
    "    elif(s=='ang'):\n",
    "        return 1\n",
    "    elif(s=='hap'):\n",
    "        return 2\n",
    "    elif(s=='sad'):\n",
    "        return 3\n",
    "    else:\n",
    "        return -1\n",
    "rootdir = '/home/varun/Downloads/SER/IEMOCAP_full_release'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector_from_mfcc(file_path: str, flatten: bool,\n",
    "                                 mfcc_len: int = 39) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Make feature vector from MFCC for the given wav file.\n",
    "    Args:\n",
    "        file_path (str): path to the .wav file that needs to be read.\n",
    "        flatten (bool) : Boolean indicating whether to flatten mfcc obtained.\n",
    "        mfcc_len (int): Number of cepestral co efficients to be consider.\n",
    "    Returns:\n",
    "        numpy.ndarray: feature vector of the wav file made from mfcc.\n",
    "    \"\"\"\n",
    "    fs, signal = wav.read(file_path)\n",
    "    s_len = len(signal)\n",
    "    # pad the signals to have same size if lesser than required\n",
    "    # else slice them\n",
    "    if s_len < mean_signal_length:\n",
    "        pad_len = mean_signal_length - s_len\n",
    "        pad_rem = pad_len % 2\n",
    "        pad_len //= 2\n",
    "        signal = np.pad(signal, (pad_len, pad_len + pad_rem),\n",
    "                        'constant', constant_values=0)\n",
    "    else:\n",
    "        pad_len = s_len - mean_signal_length\n",
    "        pad_len //= 2\n",
    "        signal = signal[pad_len:pad_len + mean_signal_length]\n",
    "    mel_coefficients = mfcc(signal, fs, num_cepstral=mfcc_len)\n",
    "    if flatten:\n",
    "        # Flatten the data\n",
    "        mel_coefficients = np.ravel(mel_coefficients)\n",
    "    return mel_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path: str, flatten: bool = True, mfcc_len: int = 39,\n",
    "             class_labels: Tuple = (\"Neutral\", \"Angry\", \"Happy\", \"Sad\")) -> \\\n",
    "        Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Extract data for training and testing.\n",
    "    1. Iterate through all the folders.\n",
    "    2. Read the audio files in each folder.\n",
    "    3. Extract Mel frequency cepestral coefficients for each file.\n",
    "    4. Generate feature vector for the audio files as required.\n",
    "    Args:\n",
    "        data_path (str): path to the data set folder\n",
    "        flatten (bool): Boolean specifying whether to flatten the data or not.\n",
    "        mfcc_len (int): Number of mfcc features to take for each frame.\n",
    "        class_labels (tuple): class labels that we care about.\n",
    "    Returns:\n",
    "        Tuple[numpy.ndarray, numpy.ndarray]: Two numpy arrays, one with mfcc and\n",
    "        other with labels.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    names = []\n",
    "    cur_dir = os.getcwd()\n",
    "    sys.stderr.write('curdir: %s\\n' % cur_dir)\n",
    "    os.chdir(data_path)\n",
    "    for i, directory in enumerate(class_labels):\n",
    "        sys.stderr.write(\"started reading folder %s\\n\" % directory)\n",
    "        os.chdir(directory)\n",
    "        for filename in os.listdir('.'):\n",
    "            filepath = os.getcwd() + '/' + filename\n",
    "            feature_vector = get_feature_vector_from_mfcc(file_path=filepath,\n",
    "                                                          mfcc_len=mfcc_len,\n",
    "                                                          flatten=flatten)\n",
    "#             print(feature_vector.shape)\n",
    "            data.append(feature_vector)\n",
    "            labels.append(i)\n",
    "            names.append(filename)\n",
    "        sys.stderr.write(\"ended reading folder %s\\n\" % directory)\n",
    "        os.chdir('..')\n",
    "    os.chdir(cur_dir)\n",
    "#     for speaker in os.listdir(rootdir):\n",
    "#         if(speaker[0] == 'S'):\n",
    "#             sub_dir = os.path.join(rootdir,speaker,'sentences/wav')\n",
    "#             emoevl = os.path.join(rootdir,speaker,'dialog/EmoEvaluation')\n",
    "#             for sess in os.listdir(sub_dir):\n",
    "#                 if(sess[7] == 'i'):\n",
    "#                     emotdir = emoevl+'/'+sess+'.txt'\n",
    "#                     #emotfile = open(emotdir)\n",
    "#                     emot_map = {}\n",
    "#                     with open(emotdir,'r') as emot_to_read:\n",
    "#                         while True:\n",
    "#                             line = emot_to_read.readline()\n",
    "#                             if not line:\n",
    "#                                 break\n",
    "#                             if(line[0] == '['):\n",
    "#                                 t = line.split()\n",
    "#                                 emot_map[t[3]] = t[4]\n",
    "                                \n",
    "        \n",
    "#                     file_dir = os.path.join(sub_dir, sess, '*.wav')\n",
    "#                     files = glob.glob(file_dir)\n",
    "#                     for filename in files:\n",
    "#                         #wavname = filename[-23:-4]\n",
    "#                         wavname = filename.split(\"/\")[-1][:-4]\n",
    "#                         emotion = emot_map[wavname]\n",
    "#                         if(emotion in ['hap','ang','neu','sad']):\n",
    "#                             filepath = filename\n",
    "#                             feature_vector = get_feature_vector_from_mfcc(file_path=filepath,\n",
    "#                                                           mfcc_len=mfcc_len,\n",
    "#                                                           flatten=flatten)\n",
    "#                             data.append(feature_vector)\n",
    "#                             labels.append(classLabels(emotion))\n",
    "#                             names.append(filename)    \n",
    "    \n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from keras.layers import  Dense, Dropout, Conv2D, Flatten,BatchNormalization, Activation, MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import merge\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "_DATA_PATH = '../SER/dataset1/'\n",
    "_CLASS_LABELS = (\"Neutral\", \"Angry\", \"Happy\", \"Sad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(flatten):\n",
    "    data, labels = get_data(_DATA_PATH, class_labels=_CLASS_LABELS,flatten=flatten)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data,labels,test_size=0.2,random_state=42)\n",
    "    return np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test), len(_CLASS_LABELS)\n",
    "\n",
    "def get_feature_vector(file_path, flatten):\n",
    "    return get_feature_vector_from_mfcc(file_path, flatten, mfcc_len=39)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(sample):\n",
    "    return np.argmax(model.predict(np.array([sample])))\n",
    "\n",
    "def predict(samples):\n",
    "    results = []\n",
    "    for _, sample in enumerate(samples):\n",
    "        results.append(predict_one(sample))\n",
    "    return tuple(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-f2a7d386b272>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-f2a7d386b272>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    class_weight = {0: 0.5.,\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class_weight = {0: 0.5.,\n",
    "                1: 1.,\n",
    "                2: 2.,\n",
    "                3: 1.}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model  = Sequential()\n",
    "model.add(Bidirectional(LSTM(128,input_shape=(198, 39))))\n",
    "\n",
    "# model = Sequential()\n",
    "#model.add(keras.layer.merge([model2, model1], mode='sum'))\n",
    "# added = keras.layers.merge.Add()([model2,model1])\n",
    "# out = keras.layers.Dense(32)(added)\n",
    "# model = keras.models.Model(inputs=[input1, input2], outputs=out)\n",
    "#added = keras.layers.Add()([x1, x2])\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='tanh'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "n_epochs = 50\n",
    "best_acc = 0\n",
    "x_train, x_test, y_train, y_test, num_labels = extract_data(flatten=False)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test_train = np_utils.to_categorical(y_test)\n",
    "x_val = x_test\n",
    "y_val = y_test_train\n",
    "\n",
    "# for i in range(n_epochs): \n",
    "    # Shuffle the data for each epoch in unison inspired\n",
    "    # from https://stackoverflow.com/a/4602224\n",
    "p = np.random.permutation(len(x_train))\n",
    "x_train = x_train[p]\n",
    "y_train = y_train[p]\n",
    "save_path1 =  './BilSTMExperimenttest.h5'\n",
    "checkpoint =  ModelCheckpoint(save_path1, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(x_train, y_train,validation_data=(x_val,y_val), batch_size=32,epochs=50,callbacks=callbacks_list)\n",
    "#print(\"Epoch Number:{}\".format(i))\n",
    "#     loss, acc = model.evaluate(x_val, y_val)\n",
    "#     if acc > best_acc:\n",
    "#         best_acc = acc\n",
    "#         model.save_weights(save_path1)\n",
    "        \n",
    "#print(type(x_test))        \n",
    "#model.evaluate(x_test, y_test)\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "predictions = predict(x_test)\n",
    "print(y_test)\n",
    "print(predictions)\n",
    "print('Accuracy:%.3f\\n' % accuracy_score(y_pred=predictions,\n",
    "                                         y_true=y_test))\n",
    "print('Confusion matrix:', confusion_matrix(y_pred=predictions,\n",
    "                                            y_true=y_test))\n",
    "print(model.summary(), file=sys.stderr)\n",
    "save_path =  'BilSTMfinaltest_model.h5'\n",
    "model.save_weights(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, num_labels = extract_data(flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done recording\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024 \n",
    "FORMAT = pyaudio.paInt16 #paInt8\n",
    "CHANNELS = 1 \n",
    "RATE = 16000 #sample rate\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"output10.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK) #buffer\n",
    "\n",
    "print(\"* recording\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data) # 2 bytes(16 bits) per channel\n",
    "\n",
    "print(\"* done recording\")\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector_from_mfcc1(file, mfcc_len: int = 39) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Make feature vector from MFCC for the given wav file.\n",
    "    Args:\n",
    "        file_path (str): path to the .wav file that needs to be read.\n",
    "        flatten (bool) : Boolean indicating whether to flatten mfcc obtained.\n",
    "        mfcc_len (int): Number of cepestral co efficients to be consider.\n",
    "    Returns:\n",
    "        numpy.ndarray: feature vector of the wav file made from mfcc.\n",
    "    \"\"\"\n",
    "    fs, signal = wav.read(file)\n",
    "    s_len = len(signal)\n",
    "    if s_len < mean_signal_length:\n",
    "        pad_len = mean_signal_length - s_len\n",
    "        pad_rem = pad_len % 2\n",
    "        pad_len //= 2\n",
    "        signal = np.pad(signal, (pad_len, pad_len + pad_rem),\n",
    "                        'constant', constant_values=0)\n",
    "    else:\n",
    "        pad_len = s_len - mean_signal_length\n",
    "        pad_len //= 2\n",
    "        signal = signal[pad_len:pad_len + mean_signal_length]\n",
    "    mel_coefficients = mfcc(signal, fs, num_cepstral=mfcc_len)\n",
    "    return mel_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('../model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"./BilSTMExperimenttest.h5\")\n",
    "# filename = '/home/varun/Downloads/speech-emotion-recognition-master/dataset/Angry/15b10Wa.wav'\n",
    "\n",
    "def pred_model(filename):\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"./BilSTMExperimenttest.h5\")\n",
    "    processedFile = get_feature_vector_from_mfcc(filename,flatten = False)\n",
    "    processedFile = np.reshape(processedFile,(1,198,39))\n",
    "    probs = loaded_model.predict(processedFile)\n",
    "    results = np.argmax(probs)\n",
    "    if(results==0):\n",
    "        emotion =  'Neutral'\n",
    "    if(results==1):\n",
    "        emotion =  'Angry' \n",
    "    if(results==2):\n",
    "        emotion =  'Happy'\n",
    "    if(results==3):\n",
    "        emotion =  'Sad'\n",
    "    return emotion,probs    \n",
    "    # print(emotion)\n",
    "    # print(pred)\n",
    "    # print('prediction', loaded_model.predict(processedFile))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './testfiles1/YAF_thought_angry.wav'\n",
    "processedFile = get_feature_vector_from_mfcc1(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/varun/Downloads/Speech Emotion Recognition'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Neutral',\n",
       " array([[0.71349746, 0.00162811, 0.01242938, 0.27244502]], dtype=float32))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = './testfiles3/YAF_mood_angry.wav'\n",
    "pred_model(filename)\n",
    "# print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "101a6ff7ecee390d2e1890350f9112fbc4081685c615de8bd7d4ce8ede852617"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
